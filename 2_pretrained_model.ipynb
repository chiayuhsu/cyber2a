{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd163c75-784f-4b23-9fd1-a1eef17b555f",
   "metadata": {},
   "source": [
    "## Pretrained models\n",
    "\n",
    "In addition to building neural network models from scratch, PyTorch provides access to a variety of pretrained models through the `torchvision` library. These models have been trained on large datasets and can be fine-tuned for specific tasks, saving significant time and computational resources. This example demonstrates how to use a pretrained ResNet-18 model and adapt it for a custom classification task with 10 classes.\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Importing Pretrained Models**:\n",
    "   - The code begins by importing the necessary modules from `torchvision`, specifically the `models` package which contains a collection of pretrained models, and `ResNet18_Weights` for accessing the pretrained weights.\n",
    "\n",
    "2. **Loading a Pretrained Model**:\n",
    "   - `model_ft = models.resnet18(weights=ResNet18_Weights.DEFAULT)` loads the ResNet-18 model with its default pretrained weights.\n",
    "   - Using pretrained weights means the model has already learned useful features from a large dataset like ImageNet.\n",
    "\n",
    "3. **Modifying the Final Layer**:\n",
    "   - Pretrained models are often used as feature extractors. The final fully connected layer of the ResNet-18 model is modified to match the number of classes in the new task (10 in this case).\n",
    "   - `num_ftrs = model_ft.fc.in_features` retrieves the number of input features to the final fully connected layer.\n",
    "   - `model_ft.fc = torch.nn.Linear(num_ftrs, 10)` replaces the original final layer with a new `Linear` layer that has 10 output features, corresponding to the number of classes in the new task.\n",
    "\n",
    "4. **Device Handling**:\n",
    "   - The code checks for the availability of a GPU and assigns the model to the appropriate device using `torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`.\n",
    "   - `model_ft = model_ft.to(device)` moves the model to the selected device, ensuring that computations are performed on the GPU if available, which accelerates the training and inference processes.\n",
    "\n",
    "### Benefits of Using Pretrained Models:\n",
    "\n",
    "- **Reduced Training Time**: Pretrained models have already learned a wide range of features, so only the final layers need to be trained or fine-tuned, significantly reducing training time.\n",
    "- **Improved Performance**: Starting with a model that already has learned representations can lead to better performance, especially with limited training data.\n",
    "- **Flexibility**: Pretrained models can be adapted to various tasks by simply modifying the output layers, making them versatile tools in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43060134-2f44-4ae6-8b45-4e3c1b8a8c08",
   "metadata": {},
   "source": [
    "### Step 1: load a pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd931385-585f-4bc0-9492-c08ca2cb9079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models # https://pytorch.org/vision/stable/models.html\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "\n",
    "# Load the pretrained ResNet18 model\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87696a-f747-4c15-ad09-50aba6642396",
   "metadata": {},
   "source": [
    "### Note\n",
    "By printing out the model, you may notice that the structure resembles a dictionary, with keys representing the names and the corresponding values being the layers. In PyTorch, you can access and print these keys (names) and values (layers) using the named_children() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864cd00b-264f-4106-90f1-2d171b18f241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 <class 'torch.nn.modules.conv.Conv2d'>\n",
      "bn1 <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "relu <class 'torch.nn.modules.activation.ReLU'>\n",
      "maxpool <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "layer1 <class 'torch.nn.modules.container.Sequential'>\n",
      "layer2 <class 'torch.nn.modules.container.Sequential'>\n",
      "layer3 <class 'torch.nn.modules.container.Sequential'>\n",
      "layer4 <class 'torch.nn.modules.container.Sequential'>\n",
      "avgpool <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "fc <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(name, type(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbd764-9130-4b4c-888d-4865406fc6ef",
   "metadata": {},
   "source": [
    "### Note\n",
    "Most layers in PyTorch represent specific operations. However, there are times when we don’t want to name every operation individually. In such cases, we can use `Sequential` to stack layers together and apply them as a single operation. For example, `layer1` can contain multiple layers, allowing for more concise and organized model definitions.\n",
    "\n",
    "Next, let's create a sample input to see how the output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248f8cf9-f785-40a7-818c-74824a08477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "sample_input = torch.randn((1, 3, 256, 256))\n",
    "\n",
    "output = model(sample_input)\n",
    "\n",
    "print(f'output shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0937f-2044-4c62-8257-d1d5b96a21e7",
   "metadata": {},
   "source": [
    "### Adapt the Output to Our Task\n",
    "\n",
    "As you can see, the output shape is \\(1 \\times 1000\\). This is because the model is pretrained on ImageNet, which has 1000 classes. We need to adapt the output for our task, which involves 10 classes, in order to fine-tune the model. \n",
    "\n",
    "If you refer back to the printed model, you’ll notice that the last layer, `fc`, has `in_features` of 512 and `out_features` of 1000. This is where the 1000-class output is generated. Therefore, we can replace this layer with a new `fc` layer that has `out_features = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109ec062-e78d-4a15-a5a9-b447855fd42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modify the final layer to match the number of classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# print the model again to observe the new `fc` layer\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e14daa-0705-4b00-a854-b2fac804c2bc",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "The new `fc` layer has only 10 `out_features`. Let's input the sample tensor again to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9031ea31-2154-43a4-8bf6-b936d980a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "output = model(sample_input)\n",
    "\n",
    "print(f'output shape: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8dd08-2e20-40eb-ab2c-1292830449ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
