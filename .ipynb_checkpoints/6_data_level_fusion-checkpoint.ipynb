{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb77c31-13b6-4295-8498-01b7f1a569af",
   "metadata": {},
   "source": [
    "## Data level fusion classification \n",
    "In data level fusion, there are essentially two key aspects to consider. First is the data itself. The dataset should be capable of outputting multi-modal data. For simplicity, we can stack the data within the dataset class. However, for better generalization, it may be more effective to implement this in your model. This approach allows you to use the same dataset class across all your multi-modal models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d969c81-be0e-43f5-bc8a-ca3465a247b5",
   "metadata": {},
   "source": [
    "## Step 1. Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82624e01-2573-4dbb-9c41-f50b2362e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, split, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split (str): One of 'train' or 'valtest' to specify the dataset split.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        # Define the directories where images are stored\n",
    "        self.rgb_dir = \"cyber2a/rts/rgb/\"\n",
    "        self.ndvi_dir = \"cyber2a/rts/ndvi/\"\n",
    "        self.nir_dir = \"cyber2a/rts/nir/\"\n",
    "\n",
    "        # Load the list of images based on the split (train/valtest)\n",
    "        with open(\"cyber2a/rts/data_split.json\") as f:\n",
    "            data_split = json.load(f)\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.img_list = data_split[\"train\"]\n",
    "        elif split == \"valtest\":\n",
    "            self.img_list = data_split[\"valtest\"]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split: choose either 'train' or 'valtest'\")\n",
    "\n",
    "        # Load the image labels\n",
    "        with open(\"cyber2a/rts/rts_cls.json\") as f:\n",
    "            self.img_labels = json.load(f)\n",
    "\n",
    "        # Store the transform to be applied to images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images.\"\"\"\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing RGB image, depth image, and label.\n",
    "        \"\"\"\n",
    "        # Retrieve the image name using the index\n",
    "        img_name = self.img_list[idx]\n",
    "\n",
    "        # Construct the full paths to the RGB and depth image files\n",
    "        rgb_path = os.path.join(self.rgb_dir, img_name)\n",
    "        ndvi_path = os.path.join(self.ndvi_dir, img_name)\n",
    "        nir_path = os.path.join(self.nir_dir, img_name)\n",
    "\n",
    "        # Open the RGB image and convert it to RGB format\n",
    "        rgb_image = cv2.imread(rgb_path)\n",
    "        rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB)\n",
    "        # Open the ndvi image\n",
    "        ndvi_image = cv2.imread(ndvi_path)[:, :, 0]\n",
    "        # Open the nir image\n",
    "        nir_image = cv2.imread(nir_path)[:, :, 0]\n",
    "\n",
    "        # normalize the rgb image\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        rgb_image = rgb_image / 255.0\n",
    "        rgb_image = (rgb_image - mean) / std\n",
    "\n",
    "        # normalize the ndvi image per sample\n",
    "        ndvi_image = (ndvi_image - ndvi_image.min()) / (\n",
    "            ndvi_image.max() - ndvi_image.min()\n",
    "        )\n",
    "\n",
    "        # normalize the nir image per sample\n",
    "        nir_image = (nir_image - nir_image.min()) / (nir_image.max() - nir_image.min())\n",
    "\n",
    "        # transfer to torch tensor\n",
    "        rgb_image = torch.tensor(rgb_image).permute(2, 0, 1)\n",
    "        ndvi_image = torch.tensor(ndvi_image).unsqueeze(0)\n",
    "        nir_image = torch.tensor(nir_image).unsqueeze(0)\n",
    "\n",
    "        image = torch.cat((rgb_image, ndvi_image, nir_image), dim=0)\n",
    "\n",
    "        # Get the corresponding label and adjust it to be 0-indexed\n",
    "        label = self.img_labels[img_name] - 1\n",
    "\n",
    "        # Apply transform if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        image = image.float()\n",
    "        \n",
    "        # Return a dictionary with all modalities\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0847b9e0-65ef-447f-a647-64a6c74c4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Define the transform for the dataset\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "])\n",
    "\n",
    "# Create the training and validation datasets with transforms\n",
    "train_dataset = MultiModalDataset(\"train\", transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "val_dataset = MultiModalDataset(\"valtest\", transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7f83f-29dd-497c-a08f-5bd61da6df86",
   "metadata": {},
   "source": [
    "## Step 2: Model\n",
    "\n",
    "The second aspect involves modifying the model's input to handle the increased data complexity. Specifically, you need to adjust the model to process data from 3 channels to 5 channels, thereby accommodating the additional information provided by the multi-modal data.\n",
    "\n",
    "This approach is similar to what we did in `2_pretrained_model.ipynb`, where we modified the last layer to produce 10 outputs for classification. Now, try modifying the first layer to accommodate a 5-channel input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d689bd-c147-4b49-9a0c-0717f960bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "\n",
    "# Load the pretrained ResNet18 model and modify the output layer to 10 classes\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 10)\n",
    "\n",
    "# expand the model to 5 channels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b743d-1a51-4cf5-b692-3b78c8f723bc",
   "metadata": {},
   "source": [
    "## Step 3. Loss and optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b620a92a-b025-4f8c-91b1-c7c2ef336707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323a2ac-5b4c-4051-a559-3e9300571dda",
   "metadata": {},
   "source": [
    "## Step 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e156cd9-0662-4352-98a5-33e7d885b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:50<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.8441408451587435\n",
      "Validation accuracy: 29.71014492753623%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:48<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 1.785018451630123\n",
      "Validation accuracy: 26.81159420289855%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:51<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 1.722829799488108\n",
      "Validation accuracy: 36.231884057971016%\n"
     ]
    }
   ],
   "source": [
    "from train import train\n",
    "\n",
    "# move model to gpu is available \n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "\n",
    "model = train(model, criterion, optimizer, train_loader, val_loader, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49798c50-b452-4e31-8410-718aed6a6625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
